{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYNpHKgoCHi3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit embedchain"
      ],
      "metadata": {
        "id": "lzdxh_7rCLig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade embedchain"
      ],
      "metadata": {
        "id": "hoTYbFyBCsY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q embedchain"
      ],
      "metadata": {
        "id": "ci1owcXLC5HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mem0ai"
      ],
      "metadata": {
        "id": "rhXP-_4iDW2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "IIV5d8qmD7Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mem0ai"
      ],
      "metadata": {
        "id": "JfdYqN56EMAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mem0 import Memory\n",
        "\n",
        "m = Memory()"
      ],
      "metadata": {
        "id": "BzIJfcNuES3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import tempfile\n",
        "from embedchain import BotAgent"
      ],
      "metadata": {
        "id": "vqefvg36CLlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from embedchain import BotAgent\n",
        "from embedchain.models import LLaMA  # or use OpenAI, Cohere, Anthropic\n",
        "# Choose your LLM\n",
        "llm = LLaMA()  # or use OpenAI, Cohere, Anthropic\n",
        "# Choose your vector database\n",
        "from embedchain.vector_stores import Weaviate\n",
        "# Create a Weaviate vector store\n",
        "vector_store = Weaviate()"
      ],
      "metadata": {
        "id": "uK4vDCq7CME4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt4all"
      ],
      "metadata": {
        "id": "NUZtUWYHHpM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\") # downloads / loads a 4.66GB LLM\n",
        "with model.chat_session():\n",
        "    print(model.generate(\"How can I run LLMs efficiently on my laptop?\", max_tokens=1024))"
      ],
      "metadata": {
        "id": "ccLqNcJDHpmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nomic import embed\n",
        "embeddings = embed.text([\"String 1\", \"String 2\"], inference_mode=\"local\")['embeddings']\n",
        "print(\"Number of embeddings created:\", len(embeddings))\n",
        "print(\"Number of dimensions per embedding:\", len(embeddings[0]))"
      ],
      "metadata": {
        "id": "mLp0h9fvIBnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_upstage"
      ],
      "metadata": {
        "id": "8RFSV7m9JyFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "RGJSHAcHKPCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "loader = PyMuPDFLoader(\"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\")\n",
        "\n",
        "data = loader.load()\n",
        "data[4]"
      ],
      "metadata": {
        "id": "7E44oxXeJywG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community\n"
      ],
      "metadata": {
        "id": "A9JTaAMRKiEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "pip install faiss-cpu"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Ohm36U3eLXqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# تحميل الكتاب باستخدام PyMuPDFLoader\n",
        "loader = PyMuPDFLoader(\"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# دمج جميع صفحات الكتاب في نص واحد\n",
        "text = \" \".join([page.page_content for page in data])\n",
        "\n",
        "# تقسيم النص إلى أجزاء متداخلة باستخدام RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200\n",
        ")\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# حفظ الأجزاء في مخزن فايس (محاكاة فقط)\n",
        "# في الواقع ، ستحتاج إلى الاتصال بواجهة برمجة تطبيقات فايس لحفظ البيانات.\n",
        "print(\"حفظ الأجزاء في مخزن فايس ...\")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"حفظ الجزء {i + 1} ...\")\n",
        "    # هنا يمكنك استخدام واجهة برمجة تطبيقات فايس لحفظ chunk\n",
        "\n",
        "print(\"تم حفظ جميع الأجزاء بنجاح.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "POj0yBq7LU1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import faiss\n",
        "\n",
        "# تحديد عدد الأبعاد لمتجهات النص\n",
        "d = 128  # يمكن تعديلها حسب احتياجاتك\n",
        "\n",
        "# إنشاء فهرس فايس\n",
        "index = faiss.IndexFlatL2(d)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "r5uIrBRjLZFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# تحميل نموذج OpenAI embeddings لتحويل النص إلى متجهات\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# تحويل أجزاء الكتاب إلى متجهات\n",
        "chunk_vectors = [embeddings.embed_query(chunk) for chunk in chunks]\n",
        "\n",
        "# إضافة المتجهات إلى فهرس فايس\n",
        "index.add(np.array(chunk_vectors).astype('float32'))\n",
        "\n",
        "# حفظ فهرس فايس\n",
        "faiss.write_index(index, \"book_index.bin\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fJu3NRGOLZcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install langchain_community\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ps40AlU3Lsym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gy0txoNGLtYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "biCwgvYlLtuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install langchain_community\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# تحديد نموذج هاججفيس\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# تحويل أجزاء الكتاب إلى متجهات\n",
        "chunk_vectors = [embeddings.embed_query(chunk) for chunk in chunks]\n",
        "\n",
        "# إضافة المتجهات إلى فهرس فايس\n",
        "index.add(np.array(chunk_vectors).astype('float32'))\n",
        "\n",
        "\n",
        "# حفظ فهرس فايس\n",
        "faiss.write_index(index, \"book_index.bin\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7VK_EkxZLuUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install sentence-transformers"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4Sxif2bSLvjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# تحميل فهرس فايس\n",
        "index = faiss.read_index(\"book_index.bin\")\n",
        "\n",
        "# تحويل النص المراد البحث عنه إلى متجه\n",
        "query_vector = embeddings.embed_query(\"What is the title of the book?\")\n",
        "\n",
        "# البحث عن المتجهات المشابهة في فهرس فايس\n",
        "D, I = index.search(np.array([query_vector]).astype('float32'), k=5)\n",
        "\n",
        "# عرض النتائج\n",
        "for i in I[0]:\n",
        "    print(chunks[i])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YJUhCQXVLafW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nt9B7tsyMPDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import faiss\n",
        "import os\n",
        "\n",
        "#Check if the index file exists\n",
        "index_file_path = \"book_index.bin\"\n",
        "if os.path.exists(index_file_path):\n",
        "  # Load the Faiss index if it exists\n",
        "  index = faiss.read_index(index_file_path)\n",
        "  print(\"Index loaded successfully.\")\n",
        "else:\n",
        "  print(f\"Error: Index file '{index_file_path}' not found. Please create the index first.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FDj2fMmTMmFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install langchain_community\n",
        "!pip install sentence-transformers"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SoKQRBEJMtv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import faiss\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "6Zt6FqGnMt7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the PDF document\n",
        "loader = PyMuPDFLoader(\"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# Combine pages into a single text\n",
        "text = \" \".join([page.page_content for page in data])\n",
        "\n",
        "# Split text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_text(text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "H3BFtQieMu8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import numpy as np\n",
        "# Initialize embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create the FAISS index\n",
        "d = embeddings.embed_query(chunks[0]).shape[0]  # Get embedding dimension\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# Convert chunks to embeddings and add to index\n",
        "chunk_vectors = [embeddings.embed_query(chunk) for chunk in chunks]\n",
        "index.add(np.array(chunk_vectors).astype('float32'))\n",
        "\n",
        "# Save the index to a file\n",
        "faiss.write_index(index, \"book_index.bin\")\n",
        "print(\"Index created and saved to book_index.bin\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "12gUPJL1MvjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "# تحميل وثيقة PDF\n",
        "loader = PyMuPDFLoader(\"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# دمج الصفحات في نص واحد\n",
        "text = \" \".join([page.page_content for page in data])\n",
        "\n",
        "# تقسيم النص إلى أجزاء متداخلة\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# تهيئة نموذج التضمين\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# إنشاء فهرس Faiss\n",
        "first_chunk_embedding = embeddings.embed_query(chunks[0])\n",
        "d = np.array(first_chunk_embedding).shape[0] # Convert to numpy array first\n",
        "\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# تحويل الأجزاء إلى تضمينات وإضافتها إلى الفهرس\n",
        "chunk_vectors = [embeddings.embed_query(chunk) for chunk in chunks]\n",
        "chunk_vectors_np = np.array(chunk_vectors).astype('float32')  # تحويل إلى مصفوفة NumPy\n",
        "index.add(chunk_vectors_np)  # إضافة إلى فهرس Faiss\n",
        "\n",
        "# حفظ الفهرس إلى ملف\n",
        "faiss.write_index(index, \"book_index.bin\")\n",
        "print(\"تم إنشاء الفهرس وحفظه في book_index.bin\")\n",
        "\n",
        "# تحميل الفهرس\n",
        "index = faiss.read_index(\"book_index.bin\")\n",
        "print(\"تم تحميل الفهرس بنجاح\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "M7UPV5TrNFof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the index\n",
        "index = faiss.read_index(\"book_index.bin\")\n",
        "print(\"Index loaded successfully\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "HUqdO_CxMwEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt4all\n",
        "!pip install faiss-cpu\n",
        "!pip install langchain_community\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "wADlch8jNxMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt4all"
      ],
      "metadata": {
        "id": "UT8e5xC4NywO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from gpt4all import GPT4All\n",
        "\n",
        "# تحميل وثيقة PDF\n",
        "# ... (كود تحميل وتحضير النص كما هو) ...\n",
        "\n",
        "# تهيئة نموذج التضمين\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# ... (كود إنشاء فهرس Faiss وحفظه كما هو) ...\n",
        "\n",
        "# تحميل نموذج GPT4All\n",
        "model = GPT4All(\"ggml-gpt4all-j-v1.3-groovy.bin\")\n",
        "\n",
        "# قالب لتوليد السؤال\n",
        "template = \"\"\"\n",
        "Given the following text from a book:\n",
        "\n",
        "{context}\n",
        "\n",
        "Generate a question that can be answered using the information in the text.\n",
        "The question should be relevant and interesting.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# دالة لإنشاء سؤال\n",
        "def generate_question(query):\n",
        "    # تحويل النص المراد البحث عنه إلى متجه\n",
        "    query_vector = embeddings.embed_query(query)\n",
        "\n",
        "    # البحث عن المتجهات المشابهة في فهرس فايس\n",
        "    D, I = index.search(np.array([query_vector]).astype('float32'), k=5)\n",
        "\n",
        "    # استخراج السياق من الأجزاء المطابقة\n",
        "    context = \" \".join([chunks[i] for i in I[0]])\n",
        "\n",
        "    # توليد السؤال باستخدام GPT4All\n",
        "    with model.chat_session():\n",
        "        question = model.generate(prompt.format(context=context), max_tokens=50)\n",
        "\n",
        "    return question\n",
        "\n",
        "# مثال على استخدام الدالة\n",
        "question = generate_question(\"What is the main character's name?\")\n",
        "\n",
        "print(question)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "AU6P2uRdNtXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BffAEdmtMfiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install langchain_community\n",
        "!pip install sentence-transformers\n",
        "!pip install gpt4all"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "eEh3Z7pZOqtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from gpt4all import GPT4All\n",
        "\n",
        "# تحميل وثيقة PDF\n",
        "loader = PyMuPDFLoader(\"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# دمج الصفحات في نص واحد\n",
        "text = \" \".join([page.page_content for page in data])\n",
        "\n",
        "# تقسيم النص إلى أجزاء متداخلة\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# تهيئة نموذج التضمين\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# إنشاء فهرس Faiss\n",
        "first_chunk_embedding = embeddings.embed_query(chunks[0])\n",
        "d = np.array(first_chunk_embedding).shape[0] # Convert to numpy array first\n",
        "\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# تحويل الأجزاء إلى تضمينات وإضافتها إلى الفهرس\n",
        "chunk_vectors = [embeddings.embed_query(chunk) for chunk in chunks]\n",
        "chunk_vectors_np = np.array(chunk_vectors).astype('float32')  # تحويل إلى مصفوفة NumPy\n",
        "index.add(chunk_vectors_np)  # إضافة إلى فهرس Faiss\n",
        "\n",
        "# حفظ الفهرس إلى ملف (اختياري)\n",
        "faiss.write_index(index, \"book_index.bin\")\n",
        "\n",
        "# تحميل نموذج GPT4All\n",
        "# model = GPT4All(\"ggml-gpt4all-j-v1.3-groovy.bin\")  # إذا كان النموذج محليًا\n",
        "\n",
        "model = GPT4All(model_name=\"/content/mistral-7b-v0.1.Q2_K.gguf\")\n",
        "model = GPT4All()  # لتحميل النموذج الافتراضي\n",
        "\n",
        "# قالب لتوليد السؤال\n",
        "template = \"\"\"\n",
        "Given the following text from a book:\n",
        "\n",
        "{context}\n",
        "\n",
        "Generate a question that can be answered using the information in the text.\n",
        "The question should be relevant and interesting.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# دالة لإنشاء سؤال\n",
        "def generate_question(query):\n",
        "    # تحويل النص المراد البحث عنه إلى متجه\n",
        "    query_vector = embeddings.embed_query(query)\n",
        "\n",
        "    # البحث عن المتجهات المشابهة في فهرس فايس\n",
        "    D, I = index.search(np.array([query_vector]).astype('float32'), k=5)\n",
        "\n",
        "    # استخراج السياق من الأجزاء المطابقة\n",
        "    context = \" \".join([chunks[i] for i in I[0]])\n",
        "\n",
        "    # توليد السؤال باستخدام GPT4All\n",
        "    with model.chat_session():\n",
        "        question = model.generate(prompt.format(context=context), max_tokens=50)\n",
        "\n",
        "    return question\n",
        "\n",
        "# مثال على استخدام الدالة\n",
        "question = generate_question(\"What is the main character's name?\")\n",
        "\n",
        "print(question)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "DG5jPDFKOq72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "model = GPT4All(model_name=\"ggml-gpt4all-j-v1.3-groovy.bin\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8GSxMlG7O7v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q2_K.gguf"
      ],
      "metadata": {
        "id": "kPVwIx0yMflf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PsAehEQnMfpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wpI-KdEqQSby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ob-frbhMQSfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cMz4blp8QSh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XntOuqgVQSkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "cxxGp6eGSGHo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cMh25Ms7Mfqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39lS9iGVMftx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hyeSU6BHMfwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pTK4zvoYMfyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# تحميل الكتاب باستخدام PyMuPDFLoader\n",
        "loader = PyMuPDFLoader(\"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# دمج جميع صفحات الكتاب في نص واحد\n",
        "text = \" \".join([page.page_content for page in data])\n",
        "\n",
        "# تقسيم النص إلى أجزاء متداخلة باستخدام RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200\n",
        ")\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# حفظ الأجزاء في مخزن فايس (محاكاة فقط)\n",
        "# في الواقع ، ستحتاج إلى الاتصال بواجهة برمجة تطبيقات فايس لحفظ البيانات.\n",
        "print(\"حفظ الأجزاء في مخزن فايس ...\")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"حفظ الجزء {i + 1} ...\")\n",
        "    # هنا يمكنك استخدام واجهة برمجة تطبيقات فايس لحفظ chunk\n",
        "\n",
        "print(\"تم حفظ جميع الأجزاء بنجاح.\")\n",
        "import faiss\n",
        "\n",
        "# تحديد عدد الأبعاد لمتجهات النص\n",
        "d = 128  # يمكن تعديلها حسب احتياجاتك\n",
        "\n",
        "# إنشاء فهرس فايس\n",
        "index = faiss.IndexFlatL2(d)\n",
        "!pip install langchain_community\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# تحديد نموذج هاججفيس\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# تحويل أجزاء الكتاب إلى متجهات\n",
        "chunk_vectors = [embeddings.embed_query(chunk) for chunk in chunks]\n",
        "\n",
        "# إضافة المتجهات إلى فهرس فايس\n",
        "index.add(np.array(chunk_vectors).astype('float32'))\n",
        "\n",
        "\n",
        "# حفظ فهرس فايس\n",
        "faiss.write_index(index, \"book_index.bin\")\n",
        "# تحميل فهرس فايس\n",
        "index = faiss.read_index(\"book_index.bin\")\n",
        "\n",
        "# تحويل النص المراد البحث عنه إلى متجه\n",
        "query_vector = embeddings.embed_query(\"What is the title of the book?\")\n",
        "\n",
        "# البحث عن المتجهات المشابهة في فهرس فايس\n",
        "D, I = index.search(np.array([query_vector]).astype('float32'), k=5)\n",
        "\n",
        "# عرض النتائج\n",
        "for i in I[0]:\n",
        "    print(chunks[i])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5KEBGOUhMgFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال ع المعالج"
      ],
      "metadata": {
        "id": "AtKSP73qWVe3"
      }
    },
    {
      "source": [
        "\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from gpt4all import GPT4All\n",
        "\n",
        "# تحميل وثيقة PDF\n",
        "loader = PyMuPDFLoader(\"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# دمج الصفحات في نص واحد\n",
        "text = \" \".join([page.page_content for page in data])\n",
        "\n",
        "# تقسيم النص إلى أجزاء متداخلة\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# تهيئة نموذج التضمين\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# إنشاء فهرس Faiss\n",
        "first_chunk_embedding = embeddings.embed_query(chunks[0])\n",
        "d = np.array(first_chunk_embedding).shape[0] # Convert to numpy array first\n",
        "\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# تحويل الأجزاء إلى تضمينات وإضافتها إلى الفهرس\n",
        "chunk_vectors = [embeddings.embed_query(chunk) for chunk in chunks]\n",
        "chunk_vectors_np = np.array(chunk_vectors).astype('float32')  # تحويل إلى مصفوفة NumPy\n",
        "index.add(chunk_vectors_np)  # إضافة إلى فهرس Faiss\n",
        "\n",
        "# حفظ الفهرس إلى ملف (اختياري)\n",
        "faiss.write_index(index, \"book_index.bin\")\n",
        "\n",
        "# تحميل نموذج GPT4All\n",
        "model = GPT4All(model_name=\"/content/mistral-7b-v0.1.Q2_K.gguf\")\n",
        "\n",
        "# قالب لتوليد السؤال\n",
        "template = \"\"\"\n",
        "Given the following text from a book:\n",
        "\n",
        "{context}\n",
        "\n",
        "Generate a question that can be answered using the information in the text.\n",
        "The question should be relevant and interesting.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# دالة لإنشاء سؤال\n",
        "def generate_question(query):\n",
        "    # تحويل النص المراد البحث عنه إلى متجه\n",
        "    query_vector = embeddings.embed_query(query)\n",
        "\n",
        "    # البحث عن المتجهات المشابهة في فهرس فايس\n",
        "    D, I = index.search(np.array([query_vector]).astype('float32'), k=5)\n",
        "\n",
        "    # استخراج السياق من الأجزاء المطابقة\n",
        "    context = \" \".join([chunks[i] for i in I[0]])\n",
        "\n",
        "    # توليد السؤال باستخدام GPT4All\n",
        "    with model.chat_session():\n",
        "        question = model.generate(prompt.format(context=context), max_tokens=50)\n",
        "\n",
        "    return question\n",
        "\n",
        "# مثال على استخدام الدالة\n",
        "question = generate_question(\"What is the main character's name?\")\n",
        "\n",
        "print(question)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "yEFKgJc6QT74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rDEMhAmkWa93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AqBFsbdoWa6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from gpt4all import GPT4All\n",
        "\n",
        "# تحميل وثيقة PDF\n",
        "loader = PyMuPDFLoader(\"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# دمج الصفحات في نص واحد\n",
        "text = \" \".join([page.page_content for page in data])\n",
        "\n",
        "# تقسيم النص إلى أجزاء متداخلة\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# تهيئة نموذج التضمين\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# إنشاء فهرس Faiss\n",
        "first_chunk_embedding = embeddings.embed_query(chunks[0])\n",
        "d = np.array(first_chunk_embedding).shape[0] # Convert to numpy array first\n",
        "\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# تحويل الأجزاء إلى تضمينات وإضافتها إلى الفهرس\n",
        "chunk_vectors = [embeddings.embed_query(chunk) for chunk in chunks]\n",
        "chunk_vectors_np = np.array(chunk_vectors).astype('float32')  # تحويل إلى مصفوفة NumPy\n",
        "index.add(chunk_vectors_np)  # إضافة إلى فهرس Faiss\n",
        "\n",
        "# حفظ الفهرس إلى ملف (اختياري)\n",
        "faiss.write_index(index, \"book_index.bin\")\n",
        "\n",
        "# تحميل نموذج GPT4All\n",
        "model = GPT4All(model_name=\"/content/mistral-7b-v0.1.Q2_K.gguf\")\n",
        "\n",
        "# قالب لتوليد السؤال\n",
        "template = \"\"\"\n",
        "Given the following text from a book:\n",
        "\n",
        "{context}\n",
        "\n",
        "Generate a question that can be answered using the information in the text.\n",
        "The question should be relevant and interesting.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# دالة لإنشاء سؤال\n",
        "def generate_question(query):\n",
        "    # تحويل النص المراد البحث عنه إلى متجه\n",
        "    query_vector = embeddings.embed_query(query)\n",
        "\n",
        "    # البحث عن المتجهات المشابهة في فهرس فايس\n",
        "    D, I = index.search(np.array([query_vector]).astype('float32'), k=5)\n",
        "\n",
        "    # استخراج السياق من الأجزاء المطابقة\n",
        "    context = \" \".join([chunks[i] for i in I[0]])\n",
        "\n",
        "    # توليد السؤال باستخدام GPT4All\n",
        "    with model.chat_session():\n",
        "        question = model.generate(prompt.format(context=context), max_tokens=512)\n",
        "\n",
        "    return question\n",
        "\n",
        "# مثال على استخدام الدالة\n",
        "question = generate_question(\"What is the topic of the book?\")\n",
        "\n",
        "print(question)"
      ],
      "metadata": {
        "id": "QuGXuTU1Wa3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from gpt4all import GPT4All\n",
        "\n",
        "# تحميل وثيقة PDF\n",
        "loader = PyMuPDFLoader(\"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# دمج الصفحات في نص واحد\n",
        "text = \" \".join([page.page_content for page in data])\n",
        "\n",
        "# تقسيم النص إلى أجزاء متداخلة\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# تهيئة نموذج التضمين\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# إنشاء فهرس Faiss\n",
        "first_chunk_embedding = embeddings.embed_query(chunks[0])\n",
        "d = np.array(first_chunk_embedding).shape[0] # Convert to numpy array first\n",
        "\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# تحويل الأجزاء إلى تضمينات وإضافتها إلى الفهرس\n",
        "chunk_vectors = [embeddings.embed_query(chunk) for chunk in chunks]\n",
        "chunk_vectors_np = np.array(chunk_vectors).astype('float32')  # تحويل إلى مصفوفة NumPy\n",
        "index.add(chunk_vectors_np)  # إضافة إلى فهرس Faiss\n",
        "\n",
        "# حفظ الفهرس إلى ملف (اختياري)\n",
        "faiss.write_index(index, \"book_index.bin\")\n",
        "\n",
        "# تحميل نموذج GPT4All\n",
        "model = GPT4All(model_name=\"/content/mistral-7b-v0.1.Q2_K.gguf\")\n",
        "\n",
        "# قالب لتوليد السؤال\n",
        "template = \"\"\"\n",
        "Given the following text from a book:\n",
        "\n",
        "{context}\n",
        "\n",
        "Generate a question that can be answered using the information in the text.\n",
        "The question should be relevant and interesting.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# دالة لإنشاء سؤال\n",
        "def generate_question(query):\n",
        "    # تحويل النص المراد البحث عنه إلى متجه\n",
        "    query_vector = embeddings.embed_query(query)\n",
        "\n",
        "    # البحث عن المتجهات المشابهة في فهرس فايس\n",
        "    D, I = index.search(np.array([query_vector]).astype('float32'), k=5)\n",
        "\n",
        "    # استخراج السياق من الأجزاء المطابقة\n",
        "    context = \" \".join([chunks[i] for i in I[0]])\n",
        "\n",
        "    # توليد السؤال باستخدام GPT4All\n",
        "    with model.chat_session():\n",
        "        question = model.generate(prompt.format(context=context), max_tokens=512)\n",
        "\n",
        "    return question\n",
        "\n",
        "# مثال على استخدام الدالة\n",
        "question = generate_question(\"What is the main problem in the story?\")\n",
        "\n",
        "print(question)"
      ],
      "metadata": {
        "id": "nGioGDVKcbvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6DMlb5ZQczhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NUHMeK_TkXUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dmpD98mSkXR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from gpt4all import GPT4All\n",
        "\n",
        "# تحميل وثيقة PDF\n",
        "loader = PyMuPDFLoader(\"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# دمج الصفحات في نص واحد\n",
        "text = \" \".join([page.page_content for page in data])\n",
        "\n",
        "# تقسيم النص إلى أجزاء متداخلة\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# تهيئة نموذج التضمين\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# إنشاء فهرس Faiss\n",
        "first_chunk_embedding = embeddings.embed_query(chunks[0])\n",
        "d = np.array(first_chunk_embedding).shape[0] # Convert to numpy array first\n",
        "\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# تحويل الأجزاء إلى تضمينات وإضافتها إلى الفهرس\n",
        "chunk_vectors = [embeddings.embed_query(chunk) for chunk in chunks]\n",
        "chunk_vectors_np = np.array(chunk_vectors).astype('float32')  # تحويل إلى مصفوفة NumPy\n",
        "index.add(chunk_vectors_np)  # إضافة إلى فهرس Faiss\n",
        "\n",
        "# حفظ الفهرس إلى ملف (اختياري)\n",
        "faiss.write_index(index, \"book_index.bin\")\n",
        "\n",
        "# تحميل نموذج GPT4All\n",
        "model = GPT4All(model_name=\"/content/mistral-7b-v0.1.Q2_K.gguf\")\n",
        "\n",
        "# قالب لتوليد السؤال\n",
        "template = \"\"\"\n",
        "Given the following text from a book:\n",
        "\n",
        "{context}\n",
        "\n",
        "Generate a question that can be answered using the information in the text.\n",
        "The question should be relevant and interesting.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# دالة لإنشاء سؤال\n",
        "def generate_question(query):\n",
        "    # تحويل النص المراد البحث عنه إلى متجه\n",
        "    query_vector = embeddings.embed_query(query)\n",
        "\n",
        "    # البحث عن المتجهات المشابهة في فهرس فايس\n",
        "    D, I = index.search(np.array([query_vector]).astype('float32'), k=5)\n",
        "\n",
        "    # استخراج السياق من الأجزاء المطابقة\n",
        "    context = \" \".join([chunks[i] for i in I[0]])\n",
        "\n",
        "    # توليد السؤال باستخدام GPT4All\n",
        "    with model.chat_session():\n",
        "        question = model.generate(prompt.format(context=context), max_tokens=512)\n",
        "\n",
        "    return question\n",
        "\n",
        "# مثال على استخدام الدالة\n",
        "question = generate_question(\"who is Grover?\")\n",
        "\n",
        "print(question)"
      ],
      "metadata": {
        "id": "Ues4_nhCkXOg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}