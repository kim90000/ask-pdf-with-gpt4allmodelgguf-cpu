{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WB2p7t1aXm2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "!pip install langchain-community\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "9frjbNtCXnJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from gpt4all import GPT4All\n",
        "\n",
        "# تحميل وثيقة PDF\n",
        "loader = PyMuPDFLoader(\"/content/q.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# دمج الصفحات في نص واحد\n",
        "text = \" \".join([page.page_content for page in data])\n",
        "\n",
        "# تقسيم النص إلى أجزاء متداخلة (تم التعديل)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100)\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# تهيئة نموذج التضمين\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# إنشاء فهرس Faiss\n",
        "first_chunk_embedding = embeddings.embed_query(chunks[0])\n",
        "d = np.array(first_chunk_embedding).shape[0]\n",
        "\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# تحويل الأجزاء إلى تضمينات وإضافتها إلى الفهرس\n",
        "chunk_vectors = [embeddings.embed_query(chunk) for chunk in chunks]\n",
        "chunk_vectors_np = np.array(chunk_vectors).astype('float32')\n",
        "index.add(chunk_vectors_np)\n",
        "\n",
        "# حفظ الفهرس إلى ملف (اختياري)\n",
        "faiss.write_index(index, \"book_index.bin\")\n",
        "\n",
        "# تحميل نموذج GPT4All\n",
        "model = GPT4All(model_name=\"/content/Sky-T1-32B-Preview-Q4_K_M.gguf\")\n",
        "\n",
        "# قالب لتوليد الإجابة (تم التعديل)\n",
        "template = \"\"\"\n",
        "Given the following text from a book:\n",
        "\n",
        "{context}\n",
        "\n",
        "Using only the provided text, directly answer the question: {question}.\n",
        "Do not use any outside information or prior knowledge.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# دالة لإنشاء الإجابة (تم التعديل)\n",
        "def generate_answer(query, k_value=5, max_tokens_value=300):\n",
        "    # تحويل النص المراد البحث عنه إلى متجه\n",
        "    query_vector = embeddings.embed_query(query)\n",
        "\n",
        "    # البحث عن المتجهات المشابهة في فهرس فايس\n",
        "    D, I = index.search(np.array([query_vector]).astype('float32'), k=k_value)\n",
        "\n",
        "    # استخراج السياق من الأجزاء المطابقة\n",
        "    context = \" \".join([chunks[i] for i in I[0]])\n",
        "\n",
        "    # توليد الإجابة باستخدام GPT4All\n",
        "    with model.chat_session():\n",
        "\n",
        "      prompt_with_instruction = prompt.format(context=context, question=query)\n",
        "      answer = model.generate(prompt_with_instruction, max_tokens=max_tokens_value)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# مثال على استخدام الدالة (تم التعديل)\n",
        "question = \"What is the book about?\"\n",
        "answer = generate_answer(question)\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "iVfpEnauPZ5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkUSXYO4pULe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGUhKw1xpWFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nm4tvJEupWIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/Sky-T1-32B-Preview-GGUF/resolve/main/Sky-T1-32B-Preview-Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "91R5KZTkpWK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggerganov/llama.cpp/releases/download/b4456/llama-b4456-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "id": "F2rgmdDLpXg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b4456-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "id": "x9Bxzqsepi-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3lH5K4bIruSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/build/bin/llama-cli\n"
      ],
      "metadata": {
        "id": "E9sxrIe8rzev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama-cli"
      ],
      "metadata": {
        "id": "KnVMI1YBr1JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/build/bin\n",
        "!./llama-cli -h"
      ],
      "metadata": {
        "id": "TnXxzSxJr8cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "P1nd8evesN4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp"
      ],
      "metadata": {
        "id": "epvXdgK_sC0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake -B build\n",
        "!cmake --build build --config Release"
      ],
      "metadata": {
        "id": "TnP0oAn4sUhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp310-cp310-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "FLS0YUjcuFqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt4all"
      ],
      "metadata": {
        "id": "MSBq6q5ttZNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "model = GPT4All(\"/content/Sky-T1-32B-Preview-Q4_K_M.gguf\") # downloads / loads a 4.66GB LLM\n",
        "with model.chat_session():\n",
        "    print(model.generate(\"what is 2+4=?\", max_tokens=124))"
      ],
      "metadata": {
        "id": "DpFchs4Itck4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python convert_hf_to_gguf.py --input_model path/to/your/model --output_model path/to/output/model.gguf\n"
      ],
      "metadata": {
        "id": "fyiM_kVtud0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/llama-gguf-split -m /path/to/your/model.gguf -o /path/to/output/directory\n"
      ],
      "metadata": {
        "id": "rvUx3y8Kudx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama.cpp/build/bin/llama-gguf-split -h"
      ],
      "metadata": {
        "id": "f7JLuJqEyXSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "29NqW-uEzKZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama.cpp/build/bin/llama-gguf-split --split-max-size N(5G) /content/Sky-T1-32B-Preview-Q4_K_M.gguf /content"
      ],
      "metadata": {
        "id": "PLWg5nE2zAzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!/content/llama.cpp/build/bin/llama-gguf-split --split-max-size 5G /content/Sky-T1-32B-Preview-Q4_K_M.gguf /content"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QelFnsrQzpF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TFYytNv8zAxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8yDAnDvXzAuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eGVwC6W7zAoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "./build/bin/llama-cli -m PATH_TO_MODEL -p \"Building a website can be done in 10 steps:\" -ngl 32"
      ],
      "metadata": {
        "id": "KgQ8Gpx7ujJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama.cpp/build/bin/llama-cli -m /content/Sky-T1-32B-Preview-Q4_K_M.gguf -p \"Building a website can be done in 10 steps:\""
      ],
      "metadata": {
        "id": "p77Rovo2udun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Who is Napoleon Bonaparte?"
      ],
      "metadata": {
        "id": "E_4CGNS0z68Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama.cpp/build/bin/llama-cli -m /content/Sky-T1-32B-Preview-Q4_K_M.gguf -p \"Who is Napoleon Bonaparte?\""
      ],
      "metadata": {
        "id": "e3ubm9lz6I3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"YOUR_HUGGING_FACE_TOKEN\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "W_qGSGIKz7jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/path/to/your/model_file\",\n",
        "    path_in_repo=\"model_file.bin\",\n",
        "    repo_id=\"your_username/your_repo_name\",\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8xH_o6M9z8gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rakmik/Sky-T1q4part"
      ],
      "metadata": {
        "id": "s098d2Ao1Abw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"XXXXXXXXXXXXXXXXXXXXXXXXXXX\")"
      ],
      "metadata": {
        "id": "hLDlbdL71SEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/content-00001-of-00004.gguf\",\n",
        "    path_in_repo=\"content-00001-of-00004.gguf\",\n",
        "    repo_id=\"rakmik/Sky-T1q4part\",\n",
        ")"
      ],
      "metadata": {
        "id": "xvpyb2YJ1QIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/content-00002-of-00004.gguf\",\n",
        "    path_in_repo=\"content-00002-of-00004.gguf\",\n",
        "    repo_id=\"rakmik/Sky-T1q4part\",\n",
        ")"
      ],
      "metadata": {
        "id": "AF8bZpo81hqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/content-00003-of-00004.gguf\",\n",
        "    path_in_repo=\"content-00003-of-00004.gguf\",\n",
        "    repo_id=\"rakmik/Sky-T1q4part\",\n",
        ")"
      ],
      "metadata": {
        "id": "24kfYxqr2MHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/content-00004-of-00004.gguf\",\n",
        "    path_in_repo=\"content-00004-of-00004.gguf\",\n",
        "    repo_id=\"rakmik/Sky-T1q4part\",\n",
        ")"
      ],
      "metadata": {
        "id": "kwHJYM4m2PXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFVnMwuN6Ojy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Vipl0Lm6Oh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dZ7Y6PJG6Ogy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G9Wbf-ET6OfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama.cpp/build/bin/llama-cli -m /content-00001-of-00004.gguf -p \"Who is Napoleon Bonaparte?\""
      ],
      "metadata": {
        "id": "0OOjcqWG6Obp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama-cli -m ./models/llama-7b/ggml-model-q4_0.gguf -c 512 -b 1024 -n 256 --keep 48"
      ],
      "metadata": {
        "id": "NsCBe5VF9VX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "./bin/llama-cli -m \"PATH_TO_MODEL\" -p \"Hi you how are you\" -n 50 -e -ngl 33 -t 4"
      ],
      "metadata": {
        "id": "JeBnC1sH9qgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama.cpp/build/bin/llama-cli -m /content-00001-of-00004.gguf"
      ],
      "metadata": {
        "id": "tA9MT-Gw9wVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama.cpp/build/bin/llama-cli -m \"/content-00001-of-00004.gguf\" -p \"Hi you how are you\" -n 50 -e -ngl 33 -t 4"
      ],
      "metadata": {
        "id": "6dU1fqq791DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zjtM_-5f99Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CLf2baQuO-La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PCgIK5_iO-JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AIbFn-etO-GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from gpt4all import GPT4All\n",
        "\n",
        "# تحميل وثيقة PDF\n",
        "loader = PyMuPDFLoader(\"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "# دمج الصفحات في نص واحد\n",
        "text = \" \".join([page.page_content for page in data])\n",
        "\n",
        "# تقسيم النص إلى أجزاء متداخلة (تم التعديل)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=150)\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# تهيئة نموذج التضمين\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# إنشاء فهرس Faiss\n",
        "first_chunk_embedding = embeddings.embed_query(chunks[0])\n",
        "d = np.array(first_chunk_embedding).shape[0]\n",
        "\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# تحويل الأجزاء إلى تضمينات وإضافتها إلى الفهرس\n",
        "chunk_vectors = [embeddings.embed_query(chunk) for chunk in chunks]\n",
        "chunk_vectors_np = np.array(chunk_vectors).astype('float32')\n",
        "index.add(chunk_vectors_np)\n",
        "\n",
        "# حفظ الفهرس إلى ملف (اختياري)\n",
        "faiss.write_index(index, \"book_index.bin\")\n",
        "\n",
        "# تحميل نموذج GPT4All\n",
        "model = GPT4All(model_name=\"/content/Sky-T1-32B-Preview-Q4_K_M.gguf\")\n",
        "\n",
        "# قالب لتوليد الإجابة (تم التعديل)\n",
        "prompt_template = \"\"\"\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "You are an assistant for answering questions using provided context.\n",
        "You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\n",
        "Question: {question}\n",
        "Context: {context}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# دالة لإنشاء الإجابة (تم التعديل)\n",
        "def generate_answer(query, k_value=3, max_tokens_value=300): #تم التعديل هنا\n",
        "    # تحويل النص المراد البحث عنه إلى متجه\n",
        "    query_vector = embeddings.embed_query(query)\n",
        "\n",
        "    # البحث عن المتجهات المشابهة في فهرس فايس\n",
        "    D, I = index.search(np.array([query_vector]).astype('float32'), k=k_value)\n",
        "\n",
        "    # استخراج السياق من الأجزاء المطابقة\n",
        "    context = \" \".join([chunks[i] for i in I[0]])\n",
        "\n",
        "    # توليد الإجابة باستخدام GPT4All\n",
        "    with model.chat_session():\n",
        "        answer = model.generate(prompt.format(context=context, question=query), max_tokens=max_tokens_value)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# مثال على استخدام الدالة (تم التعديل)\n",
        "question = \"who is Grover?\"\n",
        "answer = generate_answer(question)\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "pLKK6NJ_O-DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "McVjlM7RQUxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install langchain-community"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fJwyhPPsP-b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install faiss-cpu"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Cp2twksMPp5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FcPixgLSXimt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "!pip install langchain-community\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "m_tgHud7Xi76"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}